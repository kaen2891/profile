<!DOCTYPE html>
<!-- copy from raymin0223.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>June-Woo Kim</title>
<meta content="June-Woo Kim" name="June-Woo Kim">
<link href="./June-Woo_Kim_files/style.css" rel="stylesheet" type="text/css">
<script src="./June-Woo_Kim_files/jquery-1.11.1.min.js" type="text/javascript"></script>  
</head>


<body>
  <div class="menu"> <a href=https://www.kaen2891.com/index.html">Home</a>  <a href="https://www.kaen2891.com/index.html#publications">Publications</a>  
    <a href="https://www.kaen2891.com/index.html#projects">Projects</a> <a href="https://www.kaen2891.com/index.html#services"> Services</a>   <a href="https://www.kaen2891.com/index.html#awards"> Awards</a> 
    <a href="https://www.kaen2891.com/index.html#patents"> Patents</a> 
  </div>
  <div class="container">
    <table border="0">
      <tbody><tr>
        <td><img src="./June-Woo_Kim_files/profile.png" width="130"></td>
        <td style="width: 10px">&nbsp;</td>
        <td valign="top" width="500">
          <span class="name">June-Woo Kim</span>
          <p class="information"><br>
           Postdoctoral Research, Ph.D. <!--a href="https://sites.google.com/view/mlc-lab">MLC Lab</a --> </p>
          <!-- p class="information">Department of Artificial Intelligence, Kyungpook National University<br>
            80, Daehak-ro, Buk-gu, Daegu, 41566, Korea<br ></p -->
          <p class="information"><strong>Email</strong>: <span class="unselectable">kaen2891<span class="mock"></span>@gmail.com</span> <!-- span class="unselectable">/ kaen2891<span class="mock"></span><span class="hide">xkxkxk</span>@gmail.com</span --> </br>
	  <a href="https://drive.google.com/drive/folders/1N5m6rgKG9F5y_2fslPoSKo0R9ZkAJ-pT?usp=drive_link">CV</a>, <a href="http://www.linkedin.com/in/june-woo-kim-043374204/">Linkedin</a>, <a href="https://github.com/kaen2891/">Github</a> </p>
        </td>
      </tr>
    </tbody></table>
  <strong>Welcome to my page!</strong> I am a Postdoctoral Research at the Department of Psychiatry, Wonkwang University Hospital. My primary research focus lies in Speech Recognition and Audio in Medical AI, but I have also explored various AI domains, including LLMs, audio, and video, with the aim of expanding my understanding and skills.    
   I am particularly interested in <strong>Medical AI, including psychiatry analysis and mental health detection, respiratory sound classification</strong>, and developing an <strong>ASR system that ensures fair and unbiased speech recognition performance</strong> regardless of the speaker's personal characteristics</strong>. I'd love to explore potential collaboration with you. Please feel free to reach out to me via email if you're interested.

   <a id="news" class="anchor"></a><span class="section">News</span> 
            
    <p class="news">
     <strong>May. 2025:</strong> Two conference papers were accepted at InterSpeech 2025.
    </p>
    
    <p class="news">
     <strong>Apr. 2025:</strong> Two conference papers were accepted at IEEE EMBC 2025.
    </p>
    
    <p class="news">
     <strong>Feb. 2025:</strong> A journal paper on 'Adaptive Metadata-Guided Contrastive Learning' accepted at IEEE JBHI 2025 (IF 6.7).
    </p>
    
    <p class="news">
     <strong>Dec. 2024:</strong> A paper on 'Call-for-Help Keyword Spotting' accepted at ICASSP 2025.
    </p>
    
    
    <p>&nbsp;</p>

   <a id="education" class="anchor"></a><span class="section">Education</span> 

   <li style="line-height:160%;"> Ph.D. in Department of Artificial Intelligence, Kyungpook National University. Advised by Prof. <a href="https://scholar.google.com/citations?user=gvaE8RUAAAAJ&hl=en"><font color="#000080">Ho-Young Jung</font></a>. <em>Feb. 2025</em> </li> 
   <li style="line-height:160%;"> M.S. in Department of Artificial Intelligence, Kyungpook National University. Advised by Prof. <a href="https://scholar.google.com/citations?user=LOCg7vsAAAAJ&hl=en"><font color="#000080">Minho Lee</font></a>. <em>Feb. 2021</em> </li>
   <li style="line-height:160%;"> B.S. in Department of Information and Communication Convergence Engineering, Mokwon University. <em>Feb. 2017</em> </li>
   
   
   <a id="works" class="anchor"></a><span class="section">Work Experience</span> 
   
   <li style="line-height:160%;"> Applied Scientist Intern at Amazon; Improving Alexa shopping customers' ASR performance using synthetic speech based on TTS. Advised by Federica Cerina and Dhruv Agarwal. <em>May - Sep 2024</em> </li>

   <li style="line-height:160%;"> Research Ph.D internship at NAVER AI; Improving speech recognition performance in doctor-patient conversations utilizing speaker verification model, improving respiratory sound classification using prompted metadata as text description, psychiatry voice analysis. Advised by <a href="https://scholar.google.co.kr/citations?user=nHZWDlkAAAAJ&hl=en"><font color="#000080">Seong-Eun Moon</font></a>. <em>Jan - Apr 2024</em> </li>
	

   <p>&nbsp;</p>

    <!-- Publication session -->
    <a id="publications" class="anchor"></a><span class="section">Publications <a href="https://scholar.google.co.kr/citations?user=bMI8tY0AAAAJ&hl=en">  Google Scholar </a> </span>
    *: 1st co-authors, <sup>&dagger;</sup>: corresponding authors, C: conferences, J: journals, W: workshops, P: preprints </br> </br>
    
      <tbody><font size="4.5px"><strong>2025</strong></font>
	    <table border="0" width="90%" class="paper"><tbody>
         
         
        
        
        
        
        <!--<tr>
          <td>
            <img src="./images/2025_improving2.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [P3] <strong>June-Woo Kim</strong>, Miika Toikkanen, Kyunghoon Kim<sup>&dagger;</sup>. <strong>Improving Respiratory Sound Classification via Meta-Ensemble Learning with Diverse Data Split Strategy</strong>. <em> Preprint. </em> 
            
          </td>
        </tr> -->
        
        <tr>
          <td>
            <img src="./images/2025_arclung.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [P6] Yoon Tae Kim, Heejoon Koo, Miika Toikkanen, Soo Yong Kim, <strong>June-Woo Kim</strong><sup>&dagger;</sup>. <strong>ArcLung: Quality-Aware Margin Regularization for Respiratory Sound Classification</strong>. <em> Preprint. </em> 
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2025_empowering.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [P5] Heejoon Koo, Miika Toikkanen, Yoon Tae Kim, Soo Yong Kim, <strong>June-Woo Kim</strong><sup>&dagger;</sup>. <strong>Empowering Multimodal Respiratory Sound Classification with Counterfactual Adversarial Debiasing for Out-of-Distribution Robustness</strong>. <em> Preprint. </em> 
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_detecting.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [P4] <strong>June-Woo Kim</strong>, Haram Yoon, Wonkyo Oh, Dawoon Jung, Miika Toikkanen, Yeongdae Jo, Su-Woo Lee, Chan-Kyu Jeong, Sung-Hoon Yoon, Dae-Jin Kim, Su-in Jung, Dong-Ho Lee, Sang-Yeol Lee, Bung-Nyun Kim, Haanju Yoo, Young-Ho Kim, and Chan-Mo Yang<sup>&dagger;</sup>. <strong>Detecting Suicidal Risk Using Large Language Models in Adolescents: Integrating Patient Journaling and Psychiatric Clinical Indicators</strong>. <em> Preprint. </em>  
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        
        <tr>
          <td>
            <img src="./images/2025_improving2.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [P3] <strong>June-Woo Kim</strong>, Miika Toikkanen, Kyunghoon Kim<sup>&dagger;</sup>. <strong>Improving Respiratory Sound Classification via Meta-Ensemble Learning with Diverse Data Split Strategy</strong>. <em> Preprint. </em> 
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_improving.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [C11] Miika Toikkanen, <strong>June-Woo Kim</strong><sup>&dagger;</sup>. <strong>Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles</strong>. <em> Conference of the International Speech Communication Association </em>  (INTERSPEECH) 2025.
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_language.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [C10] <strong>June-Woo Kim</strong>, Wonkyo Oh, Haram Yoon, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang<sup>&dagger;</sup>. <strong>Language-Agnostic Suicidal Risk Detection Using Large Language Models</strong>. <em> Conference of the International Speech Communication Association </em>  (INTERSPEECH) 2025.
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_domain.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [C9] <strong>June-Woo Kim</strong>, Haram Yoon, Wonkyo Oh, Dawoon Jung, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang<sup>&dagger;</sup>. <strong>Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection</strong>. <em> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) </em> 2025.
            [<a href="https://arxiv.org/abs/2505.03359"><font color="#000080">webpage</font></a>]
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_tri.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [C8] <strong>June-Woo Kim</strong>, Sanghoon Lee, Miika Toikkanen, Daehwan Hwang, Kyunghoon Kim<sup>&dagger;</sup>. <strong>Tri-MTL: A Triple Multitask Learning Approach for Respiratory Disease Diagnosis</strong>. <em> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) </em> 2025.
            [<a href="https://www.arxiv.org/abs/2505.06271"><font color="#000080">webpage</font></a>]
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2024_amg-scl.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [J8] <strong>June-Woo Kim</strong>, Miika Toikkanen, Amin Jalali, Minseok Kim, Hye-Ji Han, Hyunwoo Kim, Wonwoo Shin, Ho-Young Jung<sup>&dagger;</sup>, Kyunghoon Kim<sup>&dagger;</sup>. <strong>Adaptive Metadata-Guided Supervised Contrastive Learning for Domain Adaptation on Respiratory Sound Classification</strong>. <em> IEEE Journal of Biomedical and Health Informatics, To be appeared </em> (JBHI) 2025.
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            [<a href="https://ieeexplore.ieee.org/document/10902164"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2024_call.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C7] Myeonghoon Ryu*<sup>&dagger;</sup>, <strong>June-Woo Kim*</strong><sup>&dagger;</sup>, Minseok Oh, Suji Lee, Han Park. <strong>Noise-Agnostic Multitask Whisper Training for Reducing False Alarm Errors in Call-for-Help Detection</strong>. <em> IEEE International Conference on Acoustics, Speech and Signal Processing </em> (ICASSP) 2025.
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2024_frechet.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [P2] <strong>June-Woo Kim*</strong>, Dhruv Agarwal*<sup>&dagger;</sup>, Federica Cerina. <strong>Understanding Frechet Speech Distance for Synthetic Speech Quality Evaluation</strong>. <em> Preprint </em>
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        </tbody></table></br>
      
      <tbody><font size="4.5px"><strong>2024</strong></font>
	    <table border="0" width="90%" class="paper"><tbody>
        
        <tr>
          <td>
            <img src="./images/2024_mdd.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [P1] <strong>June-Woo Kim*</strong>, Haram Yoon*, Bung-Nyun Kim, Sang-Yeol Lee, Dae-Jin Kim, Seong-Eun Moon, Yera Choi<sup>&dagger;</sup>, Chan-Mo Yang<sup>&dagger;</sup>. <strong>Deep Learning Analysis of Voice Biomarkers for Treatment Response in Adolescent MDD</strong>. <em> Preprint </em>
          </td>
        </tr>
        
        
        
        
        
        <tr>
          <td>
            <img src="./images/2024_decoding.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [J7] Dong-Uk Han, Subin Park, <strong>June-Woo Kim</strong>, Chan-Hyeong Lee, Ju-Hyun Bae, Ho-Young Jung, Moon-Chang Baek<sup>&dagger;</sup>, Young Ki Han<sup>&dagger;</sup>.  <strong>Multiplexed Detection Platform Implemented with Magnetic Encoding and Deep Learning-based Decoding for Quantitative Analysis of Exosomes from Cancers</strong>. <em> Sensors and Actuators B: Chemical </em> 2024.
            [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925400524009900"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2024_mad.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [J6] <strong>June-Woo Kim</strong>, Chihyeon Yoon, Ho-Young Jung<sup>&dagger;</sup>. <strong>A Military Audio Dataset for Situational Awareness and Surveillance</strong>. <em> Scientific Data </em> 2024.
            [<a href="https://github.com/kaen2891/military_audio_dataset"><font color="#000080">code</font></a>]
            [<a href="https://www.nature.com/articles/s41597-024-03511-w"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2024_bts.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
          
            [C6] <strong>June-Woo Kim</strong>, Miika Toikkanen, Yera Choi, Seong-Eun Moon<sup>&dagger;</sup>, Ho-Young Jung<sup>&dagger;</sup>.  <strong>BTS: Bridging Text and Sound Modalities for Metadata-Aided Respiratory Sound Classification</strong>. <em> Conference of the International Speech Communication Association </em>  (INTERSPEECH) 2024.
            [<a href="https://arxiv.org/abs/2406.06786"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/kaen2891/bts"><font color="#000080">code</font></a>]
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2024_repaugment.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [C5] <strong>June-Woo Kim</strong>, Miika Toikkanen, Sangmin Bae, Minseok Kim<sup>&dagger;</sup>, Ho-Young Jung<sup>&dagger;</sup>.  <strong>RepAugment: Input-Agnostic Representation-Level Augmentation for Respiratory Sound Classification</strong>. <em> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) </em> 2024.
            [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>]
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2023_sgscl.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C4] <strong>June-Woo Kim</strong>, Sangmin Bae, Won-Yang Cho, Byungjo Lee, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Stethoscope-guided Supervised Contrastive Learning for Cross-domain Adaptation on Respiratory Sound Classification</strong>. <em> IEEE International Conference on Acoustics, Speech and Signal Processing </em> (ICASSP) 2024.
            [<a href="https://arxiv.org/abs/2312.09603"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/kaen2891/stethoscope-guided_supervised_contrastive_learning"><font color="#000080">code</font></a>]
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
      </tbody></table></br>
      
      <tbody><font size="4.5px"><strong>2023</strong></font>
	    <table border="0" width="90%" class="paper"><tbody>
        
        <tr>
          <td>
            <img src="./images/2023_aft.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [W1] <strong>June-Woo Kim</strong>, C. Yoon, M. Toikkanen, S. Bae, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Adversarial Fine-tuning using Generated Respiratory Sound to Address Class Imbalance</strong>. <em>Neural Information Processing Systems Workshop on Deep Generative Models for Health</em> (NeurIPSW) 2023.
            [<a href="https://openreview.net/forum?id=z1AVG5LDQ7"><font color="#000080">webpage</font></a>]
            [<a href="https://anonymous.4open.science/r/Adversarial-Adaptation-Synthetic-Respiratory-Sound-Data-0547/README.md"><font color="#000080">demo</font></a>]
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2023_spectral.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
           [J6] <strong>June-Woo Kim</strong>, Hoon Chung, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Spectral Salt-and-Pepper Patch Masking for Self-Supervised Speech Representation Learning</strong>. <em>Mathematics</em> 2023.
            [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>]
          </td>
        </tr>   

        <tr>
          <td>
            <img src="./images/2023_patchmix.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
           [C3] Sangmin Bae*, <strong>June-Woo Kim*</strong>, Won-Yang Cho, Hyerim Baek, Soyoun Son, Byungjo Lee, Changwan Ha, Kyongpil Tae, Sungnyun Kim<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>.   <strong>Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification</strong>. <em>Conference of the International Speech Communication Association </em>  (INTERSPEECH) 2023.
            [<a href="https://www.isca-archive.org/interspeech_2023/bae23b_interspeech.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/raymin0223/patch-mix_contrastive_learning"><font color="#000080">code</font></a>]
          </td>
        </tr>   

        <tr>
          <td>
            <img src="./images/2023_unsupervised.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
           [J5] <strong>June-Woo Kim</strong>, Hoon Chung, Ho-Young Jung<sup>&dagger;</sup>.   <strong>Unsupervised Representation Learning with Task-Agnostic Feature Masking for Robust End-to-End Speech Recognition </strong>. <em>Mathematics </em> 2023.
            [<a href="https://doi.org/10.3390/math11030622"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
	</tbody></table></br>
  <font size="4.5px"><strong>2022</strong></font>
	<table border="0" width="90%" class="paper"><tbody>        

        <tr>
        <td>
            <img src="./images/2022_improved.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
           [J4] <strong>June-Woo Kim</strong>, Hyekyung Yoon, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Improved Spoken Language Representation for Intent Understanding in a Task-Oriented Dialogue System</strong>. <em>Sensors</em> 2022. 
            [<a href="https://doi.org/10.3390/s22041509"><font color="#000080">webpage</font></a>]
          </td>
        </tr>    

	</tbody></table></br>
  <font size="4.5px"><strong>2021</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
        <tr>
        <td>
            <img src="./images/2021_linguistic.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
           [J3] <strong>June-Woo Kim</strong>, Hyekyung Yoon, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Linguistic-Coupled Age-to-Age Voice Translation to Improve Speech Recognition Performance in Real Environments</strong>. <em>IEEE ACCESS </em> 2021. <!--span class="oral">Oral Presentation.</span-->
            [<a href="https://doi.org/10.1109/ACCESS.2021.3115608"><font color="#000080">webpage</font></a>]
          </td>
        </tr>  
	  
	  
	</tbody></table></br>
	<font size="4.5px"><strong>2020</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
	   
      <tr>
        <td>
          <img src="./images/2020_end.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td bgcolor="#e9eaed">
         [J2] <strong>June-Woo Kim</strong>, Ho-Young Jung<sup>&dagger;</sup>. <strong>End-to-End Speech Recognition Models using Limited Training Data</strong>. <em>Phonetics and Speech Sciences </em> 2020. 
		  [<a href="https://doi.org/10.13064/KSSS.2020.12.4.063"><font color="#000080">webpage</font></a>]
		  </td>
      </tr>

      <tr>
        <td>
          <img src="./images/2020_voice.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td>
         [J1] <strong>June-Woo Kim</strong>, Ho-Young Jung<sup>&dagger;</sup>. <strong>Voice-to-voice Conversion using Transformer Network</strong>. <em>Phonetics and Speech Sciences </em> 2020. 
          [<a href="https://doi.org/10.13064/KSSS.2020.12.3.055"><font color="#000080">webpage</font></a>]
        </td>
      </tr>
      
      <tr>
        <td>
          <img src="./images/2020_vocoder.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td bgcolor="#e9eaed">
         [C2] <strong>June-Woo Kim</strong>, Ho-Young Jung, Minho Lee<sup>&dagger;</sup>. <strong>Vocoder-free End-to-End Voice Conversion with Transformer Network</strong>. <em>International Joint Conference on Neural Networks </em> (IJCNN) 2020. 
		  [<a href="https://arxiv.org/pdf/2002.03808.pdf"><font color="#000080">pdf</font></a>]
      [<a href="https://doi.org/10.1109/IJCNN48605.2020.9207653"><font color="#000080">webpage</font></a>]
		  [<a href="https://kaen2891.github.io/voice_conversion_ijcnn2020/"><font color="#000080">demo</font></a>]
		  </td>
      </tr>
        
      
	</tbody></table></br>
	<font size="4.5px"><strong>2018</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
	  
        <tr>
        <td>
          <img src="./images/2018_end.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td>
         [C1] Myungsu Chae, Tae-Ho Kim, Younghoon Shin, <strong>June-Woo Kim</strong>, Soo-Young Lee<sup>&dagger;</sup>.  <strong>End-to-End Multimodal Emotion and Gender Recognition with Dynamic Weights of Joint Loss</strong>. <em>International Conference on Intelligent Robots and Systmes Workshop </em> (IROSW) 2018. 
          [<a href="https://arxiv.org/ftp/arxiv/papers/1809/1809.00758.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/MyungsuChae/IROS2018_ws"><font color="#000080">code</font></a>]
        </td>
      </tr>   
	  
    </tbody></table>

    <p>&nbsp;</p>

	 
    <a id="projects" class="anchor"></a><span class="section">Projects</span>

    
    <li style="line-height:160%;"> [<strong>ETRI</strong>] AI-based Broadcasting Media Editing for Content Analysis Simulator. <span class="oral">Project Manager</span>. <em>2023</em>  </li>     
    <!--
    <li style="line-height:160%;">  [<strong>NRF</strong>] Automatic Interpretation Technology Capable of Many-to-Many Multilingual Translation While Maintaining the Users' Voice Style. <span class="oral">Project Manager</span>. <em>2023-Present</em>   </li>
    -->
    <li style="line-height:160%;">  [<strong>ETRI</strong>] Unsupervised Speech Representation Learning for Robust Speech Recognition Performance. <span class="oral">Project Manager</span>. <em>2021-2024</em>  </li>     
    <li style="line-height:160%;"> [<strong>IITP</strong>] Innovative Prediction Intelligence Technology using Multimodal Information. <em>2021-2024</em>  </li>
    <li style="line-height:160%;"> [<strong>ADD</strong>] Context Awareness-based Automatic Report Generation. <em>2021-2023</em>  </li>

    <p>&nbsp;</p>

	 
    <a id="services" class="anchor"></a><span class="section">Services</span>
    <li style="line-height:160%;">  Reviewer at IEEE Transactions on Audio, Speech, and Language Processing, Scientific Reports, Artificial Intelligence Review, EURASIP Journal on Advances in Singal Processing.
    <li style="line-height:160%;">  Reviewer at IEEE ICASSP 2026-2025, Interspeech 2025, IEEE WASPAA 2025, IEEE IJCNN 2025.
    
    <p>&nbsp;</p>

	 
    <a id="awards" class="anchor"></a><span class="section">Awards and Honors</span>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from KNU Graduate Student Paper Contest in KNU. <em>2024</em></li>
    <li style="line-height:160%;"> 4th place from Human Understanding AI Paper Contest in ETRI. <em>2023</em> </li>    
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from KNU Graduate Student Paper Contest in KNU. <em>2022</em></li>
    <li style="line-height:160%;"> 7th place from Korean Speech Recognition AI Contest in Korea Ministry of Science and Technology Information and Communication. <em>2022</em></li>
    <li style="line-height:160%;"> 5th place from Human Understanding AI Paper Contest in ETRI. <em>2022</em></li>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from English Children Speech Recognition Hackathon Competition in National Information Society Agency (NIA). <em>2021</em></li>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from Korean Children Speech Recognition Hackathon Competition in National Information Society Agency (NIA). <em>2021</em></li>
    <li style="line-height:160%;"> First Prize (2th place) from ETRI AI Practice Tech Day 2021 in ETRI. <em>2021</em></li>
    <li style="line-height:160%;"> Bronze Prize (7th place) from the National Institute of Korean AI-Language Proficiency Assessment Contest. <em>2021</em> </li>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from ETRI AI Practice Tech Day 2020 in ETRI. <em>2020</em> </li>
    <li style="line-height:160%;"> <a href="https://kis.kaist.ac.kr/?document_srl=42541&mid=ki_events&sort_index=title&order_type=desc"><font color="#000080">Excellent Researcher Award</font></a> at KAIST Institute Awards in KAIST. <em>2018</em> </li>
    
    <p>&nbsp;</p>

	 
    <!--a id="patents" class="anchor"></a><span class="section">Patents</span>
    <li style="line-height:130%;"> Toward Enhanced Representation for Federated Re-Identification by Not-True Self Knowledge Distillation. S-Y. Yun, S. Kim, W. Chung, <strong>S. Bae</strong>. <em>Korea Patent Application</em>. </li>
    <li style="line-height:130%;"> Federated Learning System for Performing Individual Data Customized Federated Learning, Method for Federated Learning, and Client Aratus for Performing Same. J. Oh, S. Kim, S-Y. Yun, <strong>S. Bae</strong>, J. SHin, S. Kim, W. Chung. <em>US and Korea Patent Application</em>. </li>
    <li style="line-height:130%;"> System, Method, Computer-Readable Storage Medium and Computer Program for Federated Learning of Local Model based on Learning Direction of Global Model. G. Lee, M. Jeong, S-Y. Yun, <strong>S. Bae</strong>, J. Ahn, S. Kim, W. Chung. <em>US and Korea Patent Application</em>.     </li-->	
    
    </br>  

  <p><font color="#444444" face="Arial" size="2">&copy 2023 June-Woo Kim Thanks <a href="https://www.raymin0223.com/"><font color="#000080">Sangmin Bae</font></a> for the template. </font></p>

<!--
<div id="current_date">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.1/moment.min.js"></script>
    <script>
        const formattedDate = moment().format('YYYY-MM-DD');
        document.getElementById("current_date").innerHTML = `Last updated on: ${formattedDate}`;
    </script>
</div>
</-->

  </div>
  <script>
    var thumbnails = document.getElementsByClassName("PaperThumbnail");
    var i;
    for (i = 0; i < thumbnails.length; i++) {
      thumbnails[i].width = "120"
    }
  </script>  


</body></html>
