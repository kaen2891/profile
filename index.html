<!DOCTYPE html>
<!-- copy from raymin0223.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>June-Woo Kim</title>
<meta content="June-Woo Kim" name="June-Woo Kim">
<link href="./June-Woo_Kim_files/style.css" rel="stylesheet" type="text/css">
<script src="./June-Woo_Kim_files/jquery-1.11.1.min.js" type="text/javascript"></script>  
</head>


<body>
  <div class="menu"> <a href=https://www.kaen2891.com/index.html">Home</a>  <a href="https://www.kaen2891.com/index.html#publications">Publications</a>  
    <a href="https://www.kaen2891.com/index.html#projects">Projects</a> <a href="https://www.kaen2891.com/index.html#services"> Services</a>   <a href="https://www.kaen2891.com/index.html#awards"> Awards</a> 
    <a href="https://www.kaen2891.com/index.html#patents"> Patents</a> 
  </div>
  <div class="container">
    <table border="0">
      <tbody><tr>
        <td><img src="./June-Woo_Kim_files/profile.png" width="130"></td>
        <td style="width: 10px">&nbsp;</td>
        <td valign="top" width="500">
          <span class="name">June-Woo Kim</span>
          <p class="information"><br>
           Postdoctoral Researcher, Ph.D. <!--a href="https://sites.google.com/view/mlc-lab">MLC Lab</a --> </p>
          <!-- p class="information">Department of Artificial Intelligence, Kyungpook National University<br>
            80, Daehak-ro, Buk-gu, Daegu, 41566, Korea<br ></p -->
          <p class="information"><strong>Email</strong>: <span class="unselectable">kaen2891<span class="mock"></span>@gmail.com</span> <!-- span class="unselectable">/ kaen2891<span class="mock"></span><span class="hide">xkxkxk</span>@gmail.com</span --> </br>
	  <a href="https://scholar.google.co.kr/citations?hl=en&pli=1&user=bMI8tY0AAAAJ">Google Scholar</a>, <a href="https://drive.google.com/drive/folders/1N5m6rgKG9F5y_2fslPoSKo0R9ZkAJ-pT?usp=drive_link">CV</a>, <a href="http://www.linkedin.com/in/june-woo-kim-043374204/">Linkedin</a>, <a href="https://github.com/kaen2891/">Github</a> </p>
        </td>
      </tr>
    </tbody></table>
  <strong>Welcome to my page!</strong> I am a Postdoctoral Researcher at the GIST InnoCORE AI-Nano Convergence Institute for Early Detection of Neurodegenerative Diseases, Gwanjgu Institute of Science and Technology (GIST), Republic of Korea. My primary research focus lies in speech and signal processing in medical AI domain, but I have also explored various AI domains, including LLMs, audio, and video, with the aim of expanding my understanding and skills. I am particularly interested in <strong>medical AI, including psychiatry analysis and mental health detection, respiratory sound analysis</strong>, and developing an <strong>ASR system that ensures fair and unbiased speech recognition performance</strong> regardless of the speaker's personal characteristics</strong>. Please contact me regarding potential research collaborations in these domains! 

   <a id="news" class="anchor"></a><span class="section">News</span> 
    
    
    <p class="news">
     <strong>Dec. 2025:</strong> A journal paper on 'MDD Voice Analysis' accepted at Communications Medicine 2025 (IF 6.3).
    </p>
    
    <p class="news">
     <strong>Nov. 2025:</strong> I will serve as an Area Chair for 'ICASSP 2026'.
    </p>
            
    <p class="news">
     <strong>May. 2025:</strong> Two conference papers were accepted at Interspeech 2025.
    </p>
    
    <p class="news">
     <strong>Apr. 2025:</strong> Two conference papers were accepted at IEEE EMBC 2025.
    </p>
    
    <p class="news">
     <strong>Feb. 2025:</strong> A journal paper on 'Adaptive Metadata-Guided Contrastive Learning' accepted at IEEE JBHI 2025 (IF 6.7).
    </p>
    
    
    
    
    <p>&nbsp;</p>

   <a id="education" class="anchor"></a><span class="section">Education</span> 

   <li style="line-height:160%;"> Ph.D. in Department of Artificial Intelligence, Kyungpook National University. Advised by Prof. <a href="https://scholar.google.com/citations?user=gvaE8RUAAAAJ&hl=en"><font color="#000080">Ho-Young Jung</font></a>. <em>Feb. 2025</em> </li> 
   <li style="line-height:160%;"> M.S. in Department of Artificial Intelligence, Kyungpook National University. Advised by Prof. <a href="https://scholar.google.com/citations?user=LOCg7vsAAAAJ&hl=en"><font color="#000080">Minho Lee</font></a>. <em>Feb. 2021</em> </li>
   <li style="line-height:160%;"> B.S. in Department of Information and Communication Convergence Engineering, Mokwon University. <em>Feb. 2017</em> </li>
   
   
   <a id="works" class="anchor"></a><span class="section">Work Experience</span> 
   
   <li style="line-height:160%;"> Postdoctoral Researcher at the GIST; Multimodal representation learning using unpaired gene expression and speech data for Alzheimer's disease analysis. Advised by Prof. <a href="https://scholar.google.com/citations?user=5QyrGvsAAAAJ&hl=en"><font color="#000080">Hyunju Lee</font></a>. <em>Sep 2025 - </em> </li>
   <li style="line-height:160%;"> Postdoctoral Researcher at the Department of Psychiatry, Wonkwang University Hospital; Analysis on psychiatric domain. Advised by Prof. <a href="https://scholar.google.co.kr/citations?user=y1V7mjEAAAAJ&hl=en"><font color="#000080">Chan-Mo Yang</font></a>. <em>Mar - Sep 2025</em> </li>
   <li style="line-height:160%;"> Applied Scientist Intern II at Amazon; Improving Alexa shopping customers' ASR performance using synthetic speech based on TTS. Advised by Federica Cerina and Dhruv Agarwal. <em>May - Sep 2024</em> </li>
   <li style="line-height:160%;"> Research Ph.D internship at NAVER AI; Improving speech recognition performance in doctor-patient conversations utilizing speaker verification model, improving respiratory sound classification using prompted metadata as text description, psychiatry voice analysis. Advised by <a href="https://scholar.google.co.kr/citations?user=nHZWDlkAAAAJ&hl=en"><font color="#000080">Seong-Eun Moon</font></a>. <em>Jan - Apr 2024</em> </li>
	

   <p>&nbsp;</p>

    <!-- Publication session -->
    <a id="publications" class="anchor"></a><span class="section">Publications <a href="https://scholar.google.co.kr/citations?user=bMI8tY0AAAAJ&hl=en">  Google Scholar </a> </span>
    *: 1st co-authors, <sup>&dagger;</sup>: corresponding authors, C: conferences, J: journals, W: workshops, P: preprints </br> </br>
    
      <tbody><font size="4.5px"><strong>2025</strong></font>
	    <table border="0" width="90%" class="paper"><tbody>
         
         
        
        
        
        
        <!--<tr>
          <td>
            <img src="./images/2025_improving2.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [P3] <strong>June-Woo Kim</strong>, Miika Toikkanen, Kyunghoon Kim<sup>&dagger;</sup>. <strong>Improving Respiratory Sound Classification via Meta-Ensemble Learning with Diverse Data Split Strategy</strong>. <em> Preprint. </em> 
            
          </td>
        </tr> -->
        
        
        <tr>
          <td>
            <img src="./images/2025_psychological.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
          
            [P8] Sung Hoon Yoon, Dawoon Jung, Dae-Jin Kim, Chan-Mo Yang, Young Hyeon Ahn, <strong>June-Woo Kim</strong>, Yong-Sung Kim, Suck-Chei Choi, Sang-Yeol Lee<sup>&dagger;</sup>. <strong>Psychological Predictors of Symptom Severity in Disorders of Gut?Brain Interaction: A Machine Learning Approach</strong>. <em> Preprint. </em> 
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2024_mdd.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [J9] <strong>June-Woo Kim*</strong>, Haram Yoon*, Bung-Nyun Kim, Sang-Yeol Lee, Dae-Jin Kim, Seong-Eun Moon, Yera Choi<sup>&dagger;</sup>, Chan-Mo Yang<sup>&dagger;</sup>. <strong>Deep Neural Network-Based Analysis of Voice Biomarkers for Monitoring Treatment Response in Adolescent Major Depressive Disorder</strong>. <em> Communications Medicine, Accepted. </em> 
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2025_exploring.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [P7] <strong>June-Woo Kim</strong>, Miika Toikkanen, Heejoon Koo, Yoon Tae Kim, Won-Yang Cho, Hyesun Chang, Jaeyong Kim, Daehwan Hwang, Kyunghoon Kim<sup>&dagger;</sup>. <strong>Evaluating Generalization Strategies for Respiratory Sound Classification Across Heterogeneous Clinical Datasets: Algorithm Development and Multi-Cohort Validation</strong>. <em> Preprint. </em> 
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_dsm.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [P6] <strong>June-Woo Kim</strong>, Haram Yoon, Dawoon Jung, Wonkyo Oh, Miika Toikkanen, Dahyeon Choi, Sung-Hoon Yoon, Dae-Jin Kim, Sang-Yeol Lee, Bung-Nyun Kim, Hyunju Lee, Chan-Mo Yang<sup>&dagger;</sup>. <strong>DSM-Guided Reasoning with Large Language Models: Improving Clinical Reliability in Mental Health Assessment</strong>. <em> Preprint. </em>  
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        
        <tr>
          <td>
            <img src="./images/2025_arclung.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [P5] Yoon Tae Kim, Heejoon Koo, Miika Toikkanen, Soo Yong Kim, <strong>June-Woo Kim</strong><sup>&dagger;</sup>. <strong>ArcLung: Quality-Aware Margin Regularization for Respiratory Sound Classification</strong>. <em> Preprint. </em> 
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2025_empowering.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [P4] Heejoon Koo, Miika Toikkanen, Yoon Tae Kim, Soo Yong Kim, <strong>June-Woo Kim</strong><sup>&dagger;</sup>. <strong>Empowering Multimodal Respiratory Sound Classification with Counterfactual Adversarial Debiasing for Out-of-Distribution Robustness</strong>. <em> Preprint. </em> 
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_detecting.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [P3] <strong>June-Woo Kim</strong>, Haram Yoon, Wonkyo Oh, Dawoon Jung, Miika Toikkanen, Yeongdae Jo, Su-Woo Lee, Chan-Kyu Jeong, Sung-Hoon Yoon, Dae-Jin Kim, Su-in Jung, Dong-Ho Lee, Sang-Yeol Lee, Bung-Nyun Kim, Haanju Yoo, Young-Ho Kim, Chan-Mo Yang<sup>&dagger;</sup>. <strong>Detecting Suicidal Risk Using Large Language Models in Adolescents: Integrating Patient Journaling and Psychiatric Clinical Indicators</strong>. <em> Preprint. </em>  
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        
        <tr>
          <td>
            <img src="./images/2025_improving2.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [P2] <strong>June-Woo Kim</strong>, Miika Toikkanen, Kyunghoon Kim<sup>&dagger;</sup>. <strong>Improving Respiratory Sound Classification via Meta-Ensemble Learning with Diverse Data Split Strategy</strong>. <em> Preprint. </em> 
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_improving.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [C11] Miika Toikkanen, <strong>June-Woo Kim</strong><sup>&dagger;</sup>. <strong>Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles</strong>. <em> Conference of the International Speech Communication Association (INTERSPEECH) </em> 2025.
            [<a href="https://www.isca-archive.org/interspeech_2025/toikkanen25_interspeech.html"><font color="#000080">webpage</font></a>]
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_language.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [C10] <strong>June-Woo Kim</strong>, Wonkyo Oh, Haram Yoon, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang<sup>&dagger;</sup>. <strong>Language-Agnostic Suicidal Risk Detection Using Large Language Models</strong>. <em> Conference of the International Speech Communication Association (INTERSPEECH) </em> 2025.
            [<a href="https://www.isca-archive.org/interspeech_2025/kim25p_interspeech.html"><font color="#000080">webpage</font></a>]
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_domain.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [C9] <strong>June-Woo Kim</strong>, Haram Yoon, Wonkyo Oh, Dawoon Jung, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang<sup>&dagger;</sup>. <strong>Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection</strong>. <em> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) </em> 2025.
            [<a href="https://arxiv.org/abs/2505.03359"><font color="#000080">webpage</font></a>]
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2025_tri.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [C8] <strong>June-Woo Kim</strong>, Sanghoon Lee, Miika Toikkanen, Daehwan Hwang, Kyunghoon Kim<sup>&dagger;</sup>. <strong>Tri-MTL: A Triple Multitask Learning Approach for Respiratory Disease Diagnosis</strong>. <em> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) </em> 2025.
            [<a href="https://www.arxiv.org/abs/2505.06271"><font color="#000080">webpage</font></a>]
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2024_amg-scl.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          
          <td bgcolor="#e9eaed">
            [J8] <strong>June-Woo Kim</strong>, Miika Toikkanen, Amin Jalali, Minseok Kim, Hye-Ji Han, Hyunwoo Kim, Wonwoo Shin, Ho-Young Jung<sup>&dagger;</sup>, Kyunghoon Kim<sup>&dagger;</sup>. <strong>Adaptive Metadata-Guided Supervised Contrastive Learning for Domain Adaptation on Respiratory Sound Classification</strong>. <em> IEEE Journal of Biomedical and Health Informatics (JBHI) </em> 2025.
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            [<a href="https://ieeexplore.ieee.org/document/10902164"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2024_call.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C7] Myeonghoon Ryu*<sup>&dagger;</sup>, <strong>June-Woo Kim*</strong><sup>&dagger;</sup>, Minseok Oh, Suji Lee, Han Park. <strong>Noise-Agnostic Multitask Whisper Training for Reducing False Alarm Errors in Call-for-Help Detection</strong>. <em> IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> 2025.
            [<a href="https://ieeexplore.ieee.org/abstract/document/10890654"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2024_frechet.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          
          <td bgcolor="#e9eaed">
            [P1] <strong>June-Woo Kim</strong>, Dhruv Agarwal<sup>&dagger;</sup>, Federica Cerina. <strong>Understanding Frechet Speech Distance for Synthetic Speech Quality Evaluation</strong>. <em> Preprint </em>
            <!-- [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>] -->
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        </tbody></table></br>
      
      <tbody><font size="4.5px"><strong>2024</strong></font>
	    <table border="0" width="90%" class="paper"><tbody>
        
        
        
        
        
        
        
        <tr>
          <td>
            <img src="./images/2024_decoding.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td>
            [J7] Dong-Uk Han, Subin Park, <strong>June-Woo Kim</strong>, Chan-Hyeong Lee, Ju-Hyun Bae, Ho-Young Jung, Moon-Chang Baek<sup>&dagger;</sup>, Young Ki Han<sup>&dagger;</sup>.  <strong>Multiplexed Detection Platform Implemented with Magnetic Encoding and Deep Learning-based Decoding for Quantitative Analysis of Exosomes from Cancers</strong>. <em> Sensors and Actuators B: Chemical </em> 2024.
            [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925400524009900"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2024_mad.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [J6] <strong>June-Woo Kim</strong>, Chihyeon Yoon, Ho-Young Jung<sup>&dagger;</sup>. <strong>A Military Audio Dataset for Situational Awareness and Surveillance</strong>. <em> Scientific Data </em> 2024.
            [<a href="https://github.com/kaen2891/military_audio_dataset"><font color="#000080">code</font></a>]
            [<a href="https://www.nature.com/articles/s41597-024-03511-w"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2024_bts.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
          
            [C6] <strong>June-Woo Kim</strong>, Miika Toikkanen, Yera Choi, Seong-Eun Moon<sup>&dagger;</sup>, Ho-Young Jung<sup>&dagger;</sup>.  <strong>BTS: Bridging Text and Sound Modalities for Metadata-Aided Respiratory Sound Classification</strong>. <em> Conference of the International Speech Communication Association (INTERSPEECH)</em> 2024.
            [<a href="https://www.isca-archive.org/interspeech_2024/kim24f_interspeech.html"><font color="#000080">webpage</font></a>]
            [<a href="https://github.com/kaen2891/bts"><font color="#000080">code</font></a>]
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2024_repaugment.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [C5] <strong>June-Woo Kim</strong>, Miika Toikkanen, Sangmin Bae, Minseok Kim<sup>&dagger;</sup>, Ho-Young Jung<sup>&dagger;</sup>.  <strong>RepAugment: Input-Agnostic Representation-Level Augmentation for Respiratory Sound Classification</strong>. <em> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) </em> 2024.
            [<a href="https://ieeexplore.ieee.org/abstract/document/10782363"><font color="#000080">webpage</font></a>]
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2023_sgscl.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C4] <strong>June-Woo Kim</strong>, Sangmin Bae, Won-Yang Cho, Byungjo Lee, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Stethoscope-guided Supervised Contrastive Learning for Cross-domain Adaptation on Respiratory Sound Classification</strong>. <em> IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> 2024.
            [<a href="https://ieeexplore.ieee.org/abstract/document/10447734"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/kaen2891/stethoscope-guided_supervised_contrastive_learning"><font color="#000080">code</font></a>]
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
      </tbody></table></br>
      
      <tbody><font size="4.5px"><strong>2023</strong></font>
	    <table border="0" width="90%" class="paper"><tbody>
        
        <tr>
          <td>
            <img src="./images/2023_aft.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [W1] <strong>June-Woo Kim</strong>, C. Yoon, M. Toikkanen, S. Bae, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Adversarial Fine-tuning using Generated Respiratory Sound to Address Class Imbalance</strong>. <em>Neural Information Processing Systems Workshop on Deep Generative Models for Health (NeurIPSW)</em> 2023.
            [<a href="https://openreview.net/forum?id=z1AVG5LDQ7"><font color="#000080">webpage</font></a>]
            [<a href="https://anonymous.4open.science/r/Adversarial-Adaptation-Synthetic-Respiratory-Sound-Data-0547/README.md"><font color="#000080">demo</font></a>]
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2023_spectral.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
           [J6] <strong>June-Woo Kim</strong>, Hoon Chung, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Spectral Salt-and-Pepper Patch Masking for Self-Supervised Speech Representation Learning</strong>. <em>Mathematics</em> 2023.
            [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>]
          </td>
        </tr>   

        <tr>
          <td>
            <img src="./images/2023_patchmix.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
           [C3] Sangmin Bae*, <strong>June-Woo Kim*</strong>, Won-Yang Cho, Hyerim Baek, Soyoun Son, Byungjo Lee, Changwan Ha, Kyongpil Tae, Sungnyun Kim<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>.   <strong>Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification</strong>. <em>Conference of the International Speech Communication Association (INTERSPEECH)</em> 2023.
            [<a href="https://www.isca-archive.org/interspeech_2023/bae23b_interspeech.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/raymin0223/patch-mix_contrastive_learning"><font color="#000080">code</font></a>]
          </td>
        </tr>   

        <tr>
          <td>
            <img src="./images/2023_unsupervised.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
           [J5] <strong>June-Woo Kim</strong>, Hoon Chung, Ho-Young Jung<sup>&dagger;</sup>.   <strong>Unsupervised Representation Learning with Task-Agnostic Feature Masking for Robust End-to-End Speech Recognition </strong>. <em>Mathematics </em> 2023.
            [<a href="https://doi.org/10.3390/math11030622"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
	</tbody></table></br>
  <font size="4.5px"><strong>2022</strong></font>
	<table border="0" width="90%" class="paper"><tbody>        

        <tr>
        <td>
            <img src="./images/2022_improved.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
           [J4] <strong>June-Woo Kim</strong>, Hyekyung Yoon, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Improved Spoken Language Representation for Intent Understanding in a Task-Oriented Dialogue System</strong>. <em>Sensors</em> 2022. 
            [<a href="https://doi.org/10.3390/s22041509"><font color="#000080">webpage</font></a>]
          </td>
        </tr>    

	</tbody></table></br>
  <font size="4.5px"><strong>2021</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
        <tr>
        <td>
            <img src="./images/2021_linguistic.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
           [J3] <strong>June-Woo Kim</strong>, Hyekyung Yoon, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Linguistic-Coupled Age-to-Age Voice Translation to Improve Speech Recognition Performance in Real Environments</strong>. <em>IEEE ACCESS </em> 2021. <!--span class="oral">Oral Presentation.</span-->
            [<a href="https://doi.org/10.1109/ACCESS.2021.3115608"><font color="#000080">webpage</font></a>]
          </td>
        </tr>  
	  
	  
	</tbody></table></br>
	<font size="4.5px"><strong>2020</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
	   
      <tr>
        <td>
          <img src="./images/2020_end.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td bgcolor="#e9eaed">
         [J2] <strong>June-Woo Kim</strong>, Ho-Young Jung<sup>&dagger;</sup>. <strong>End-to-End Speech Recognition Models using Limited Training Data</strong>. <em>Phonetics and Speech Sciences </em> 2020. 
		  [<a href="https://doi.org/10.13064/KSSS.2020.12.4.063"><font color="#000080">webpage</font></a>]
		  </td>
      </tr>

      <tr>
        <td>
          <img src="./images/2020_voice.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td>
         [J1] <strong>June-Woo Kim</strong>, Ho-Young Jung<sup>&dagger;</sup>. <strong>Voice-to-voice Conversion using Transformer Network</strong>. <em>Phonetics and Speech Sciences </em> 2020. 
          [<a href="https://doi.org/10.13064/KSSS.2020.12.3.055"><font color="#000080">webpage</font></a>]
        </td>
      </tr>
      
      <tr>
        <td>
          <img src="./images/2020_vocoder.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td bgcolor="#e9eaed">
         [C2] <strong>June-Woo Kim</strong>, Ho-Young Jung, Minho Lee<sup>&dagger;</sup>. <strong>Vocoder-free End-to-End Voice Conversion with Transformer Network</strong>. <em>International Joint Conference on Neural Networks </em> (IJCNN) 2020. 
		  [<a href="https://arxiv.org/pdf/2002.03808.pdf"><font color="#000080">pdf</font></a>]
      [<a href="https://doi.org/10.1109/IJCNN48605.2020.9207653"><font color="#000080">webpage</font></a>]
		  [<a href="https://kaen2891.github.io/voice_conversion_ijcnn2020/"><font color="#000080">demo</font></a>]
		  </td>
      </tr>
        
      
	</tbody></table></br>
	<font size="4.5px"><strong>2018</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
	  
        <tr>
        <td>
          <img src="./images/2018_end.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td>
         [C1] Myungsu Chae, Tae-Ho Kim, Younghoon Shin, <strong>June-Woo Kim</strong>, Soo-Young Lee<sup>&dagger;</sup>.  <strong>End-to-End Multimodal Emotion and Gender Recognition with Dynamic Weights of Joint Loss</strong>. <em>International Conference on Intelligent Robots and Systmes Workshop </em> (IROSW) 2018. 
          [<a href="https://arxiv.org/ftp/arxiv/papers/1809/1809.00758.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/MyungsuChae/IROS2018_ws"><font color="#000080">code</font></a>]
        </td>
      </tr>   
	  
    </tbody></table>

    <p>&nbsp;</p>

	 
    <a id="projects" class="anchor"></a><span class="section">Projects</span>

    
    <li style="line-height:160%;"> [<strong>ETRI</strong>] AI-based Broadcasting Media Editing for Content Analysis Simulator. <span class="oral">Project Manager</span>. <em>2023</em>  </li>     
    <!--
    <li style="line-height:160%;">  [<strong>NRF</strong>] Automatic Interpretation Technology Capable of Many-to-Many Multilingual Translation While Maintaining the Users' Voice Style. <span class="oral">Project Manager</span>. <em>2023-Present</em>   </li>
    -->
    <li style="line-height:160%;">  [<strong>ETRI</strong>] Unsupervised Speech Representation Learning for Robust Speech Recognition Performance. <span class="oral">Project Manager</span>. <em>2021-2024</em>  </li>     
    <li style="line-height:160%;"> [<strong>IITP</strong>] Innovative Prediction Intelligence Technology using Multimodal Information. <em>2021-2024</em>  </li>
    <li style="line-height:160%;"> [<strong>ADD</strong>] Context Awareness-based Automatic Report Generation. <em>2021-2023</em>  </li>

    <p>&nbsp;</p>

	 
    <a id="services" class="anchor"></a><span class="section">Services</span>
    <li style="line-height:160%;">  Reviewer at IEEE Transactions on Audio, Speech, and Language Processing, Scientific Reports, Artificial Intelligence Review, EURASIP Journal on Advances in Singal Processing.
    <li style="line-height:160%;">  Reviewer at IEEE ICASSP 2026-2025, Interspeech 2025, IEEE WASPAA 2025, IEEE IJCNN 2025.
    
    <p>&nbsp;</p>

	 
    <a id="awards" class="anchor"></a><span class="section">Awards and Honors</span>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from KNU Graduate Student Paper Contest in KNU. <em>2024</em></li>
    <li style="line-height:160%;"> 4th place from Human Understanding AI Paper Contest in ETRI. <em>2023</em> </li>    
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from KNU Graduate Student Paper Contest in KNU. <em>2022</em></li>
    <li style="line-height:160%;"> 7th place from Korean Speech Recognition AI Contest in Korea Ministry of Science and Technology Information and Communication. <em>2022</em></li>
    <li style="line-height:160%;"> 5th place from Human Understanding AI Paper Contest in ETRI. <em>2022</em></li>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from English Children Speech Recognition Hackathon Competition in National Information Society Agency (NIA). <em>2021</em></li>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from Korean Children Speech Recognition Hackathon Competition in National Information Society Agency (NIA). <em>2021</em></li>
    <li style="line-height:160%;"> First Prize (2th place) from ETRI AI Practice Tech Day 2021 in ETRI. <em>2021</em></li>
    <li style="line-height:160%;"> Bronze Prize (7th place) from the National Institute of Korean AI-Language Proficiency Assessment Contest. <em>2021</em> </li>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from ETRI AI Practice Tech Day 2020 in ETRI. <em>2020</em> </li>
    <li style="line-height:160%;"> <a href="https://kis.kaist.ac.kr/?document_srl=42541&mid=ki_events&sort_index=title&order_type=desc"><font color="#000080">Excellent Researcher Award</font></a> at KAIST Institute Awards in KAIST. <em>2018</em> </li>
    
    <p>&nbsp;</p>

	 
    <!--a id="patents" class="anchor"></a><span class="section">Patents</span>
    <li style="line-height:130%;"> Toward Enhanced Representation for Federated Re-Identification by Not-True Self Knowledge Distillation. S-Y. Yun, S. Kim, W. Chung, <strong>S. Bae</strong>. <em>Korea Patent Application</em>. </li>
    <li style="line-height:130%;"> Federated Learning System for Performing Individual Data Customized Federated Learning, Method for Federated Learning, and Client Aratus for Performing Same. J. Oh, S. Kim, S-Y. Yun, <strong>S. Bae</strong>, J. SHin, S. Kim, W. Chung. <em>US and Korea Patent Application</em>. </li>
    <li style="line-height:130%;"> System, Method, Computer-Readable Storage Medium and Computer Program for Federated Learning of Local Model based on Learning Direction of Global Model. G. Lee, M. Jeong, S-Y. Yun, <strong>S. Bae</strong>, J. Ahn, S. Kim, W. Chung. <em>US and Korea Patent Application</em>.     </li-->	
    
    </br>  

  <p><font color="#444444" face="Arial" size="2">&copy 2023 June-Woo Kim Thanks <a href="https://www.raymin0223.com/"><font color="#000080">Sangmin Bae</font></a> for the template. </font></p>

<!--
<div id="current_date">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.1/moment.min.js"></script>
    <script>
        const formattedDate = moment().format('YYYY-MM-DD');
        document.getElementById("current_date").innerHTML = `Last updated on: ${formattedDate}`;
    </script>
</div>
</-->

  </div>
  <script>
    var thumbnails = document.getElementsByClassName("PaperThumbnail");
    var i;
    for (i = 0; i < thumbnails.length; i++) {
      thumbnails[i].width = "120"
    }
  </script>
  
  <script type="text/javascript" id="clustrmaps"
    src="//clustrmaps.com/map_v2.js?d=XXXXXXXX&cl=ffffff&w=a">
  </script>  


</body></html>
